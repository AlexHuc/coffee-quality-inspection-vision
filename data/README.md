# Data Documentation - Coffee Bean Defect Detection

This directory contains the coffee bean dataset organized for model training and evaluation. Data is split into two folders: `raw/` (original dataset) and `processed/` (split into train/test sets).

## ðŸ“‚ Directory Structure

```
data/
â”œâ”€â”€ raw/                                     # Original unmodified images from source
â”‚   â”œâ”€â”€ Broken/                              # 17 defect categories
â”‚   â”œâ”€â”€ Cut/
â”‚   â”œâ”€â”€ Dry Cherry/
â”‚   â”œâ”€â”€ Fade/
â”‚   â”œâ”€â”€ Floater/
â”‚   â”œâ”€â”€ Full Black/
â”‚   â”œâ”€â”€ Full Sour/
â”‚   â”œâ”€â”€ Fungus Damange/
â”‚   â”œâ”€â”€ Husk/
â”‚   â”œâ”€â”€ Immature/
â”‚   â”œâ”€â”€ Parchment/
â”‚   â”œâ”€â”€ Partial Black/
â”‚   â”œâ”€â”€ Partial Sour/
â”‚   â”œâ”€â”€ Severe Insect Damange/
â”‚   â”œâ”€â”€ Shell/
â”‚   â”œâ”€â”€ Slight Insect Damage/
â”‚   â””â”€â”€ Withered/
â”‚
â””â”€â”€ processed/                               # Train/Test split with standardized format
    â”œâ”€â”€ train/                               # Training dataset
    â”‚   â”œâ”€â”€ Broken/
    â”‚   â”œâ”€â”€ Cut/
    â”‚   â”œâ”€â”€ Dry Cherry/
    â”‚   â”œâ”€â”€ ... (all 17 defect classes)
    â”‚   â””â”€â”€ [normalized, resized]
    â”‚
    â””â”€â”€ test/                                # Testing dataset
        â”œâ”€â”€ Broken/
        â”œâ”€â”€ Cut/
        â”œâ”€â”€ Dry Cherry/
        â”œâ”€â”€ ... (all 17 defect classes)
        â””â”€â”€ [normalized, resized]
```

---

## ðŸ“Š Data Organization

### Raw Data (`raw/` folder)

The `raw/` folder contains **original, unmodified images** directly from the source dataset.

#### Characteristics:
- **Format**: JPG image files
- **Resolution**: 500Ã—500 pixels
- **File Naming**: `{ClassName}_{Number}.jpg` (e.g., `Broken_01.jpg`, `Broken_02.jpg`)
- **Organization**: One subfolder per defect class (17 categories)
- **Color Space**: RGB (3 channels)
- **Defect Classes**: 17 categories (Broken, Cut, Dry Cherry, Fade, Floater, Full Black, Full Sour, Fungus Damange, Husk, Immature, Parchment, Partial Black, Partial Sour, Severe Insect Damange, Shell, Slight Insect Damage, Withered)

#### Purpose:
- **Single Source of Truth**: Original dataset preserved in pristine form
- **Reproducibility**: Enables verification against source data
- **Flexibility**: Allows regeneration of processed data if needed
- **Auditability**: Maintains data lineage and integrity
- **Backup Reference**: Original source for recovery if needed

#### Raw Data Classes:
```
raw/
â”œâ”€â”€ Broken/                   # Fractured or split beans
â”œâ”€â”€ Cut/                      # Beans with cuts or nicks
â”œâ”€â”€ Dry Cherry/               # Incompletely processed beans
â”œâ”€â”€ Fade/                     # Discolored beans lacking vibrancy
â”œâ”€â”€ Floater/                  # Low-density beans that float
â”œâ”€â”€ Full Black/               # Completely dark/black beans
â”œâ”€â”€ Full Sour/                # Fermented beans with sour defects
â”œâ”€â”€ Fungus Damange/           # Mold or fungal infections
â”œâ”€â”€ Husk/                     # Beans with parchment/husk
â”œâ”€â”€ Immature/                 # Under-ripened beans
â”œâ”€â”€ Parchment/                # Incompletely removed parchment
â”œâ”€â”€ Partial Black/            # Partially dark beans
â”œâ”€â”€ Partial Sour/             # Partially fermented beans
â”œâ”€â”€ Severe Insect Damange/    # Heavy pest damage
â”œâ”€â”€ Shell/                    # Incomplete bean shells
â”œâ”€â”€ Slight Insect Damage/     # Minor pest marks
â””â”€â”€ Withered/                 # Dried/shriveled beans
```

#### Example Raw File Structure:
```
raw/Broken/
â”œâ”€â”€ Broken_01.jpg            # 500Ã—500 px, original quality
â”œâ”€â”€ Broken_02.jpg
â”œâ”€â”€ Broken_03.jpg
â”œâ”€â”€ Broken_04.jpg
â”œâ”€â”€ Broken_05.jpg
â””â”€â”€ ... (multiple images per class)

raw/Cut/
â”œâ”€â”€ Cut_01.jpg
â”œâ”€â”€ Cut_02.jpg
â””â”€â”€ ...
```

---

### Processed Data (`processed/` folder)

The `processed/` folder contains **train/test split data** with standardized preprocessing applied.

#### Structure:

1. **Train Set** (`processed/train/`)
   - Contains ~80% of images from each class
   - Used for model training
   - Organized by defect class
   - Undergoes data augmentation during training

2. **Test Set** (`processed/test/`)
   - Contains ~20% of images from each class
   - Used for model evaluation
   - Organized by defect class
   - No augmentation (true evaluation)

#### Preprocessing Applied:

1. **Train/Test Split**
   - ~80/20 split from original raw data
   - Stratified sampling (maintains class balance)
   - Ensures reproducibility

2. **Image Standardization**
   - All images resized to **224Ã—224 pixels**
   - Maintains aspect ratio with padding if needed
   - Consistent input size for neural networks

3. **Format Consistency**
   - All images in JPG format
   - Consistent quality encoding
   - Standardized file naming

4. **Color Space**
   - All images RGB (3 channels)
   - Normalized to [0, 255] range
   - Ready for normalization in training

#### Characteristics:
- **Format**: JPG (standardized)
- **Resolution**: 224Ã—224 pixels (fixed)
- **Organization**: train/ and test/ folders with 17 class subfolders each
- **Naming**: Maintains original `{ClassName}_{Number}.jpg` format
- **Color Space**: RGB (3 channels)
- **Use Case**: Direct input for model training/evaluation

#### Purpose:
- **Model Training**: Optimized for neural network input pipeline
- **Consistency**: All images have identical properties and size
- **Efficiency**: Preprocessing done once, not during each epoch
- **Reproducibility**: Consistent preprocessing ensures repeatable results
- **Faster Training**: Reduced I/O overhead with standardized format

#### Example Processed File Structure:
```
processed/train/Broken/
â”œâ”€â”€ Broken_01.jpg            # 224Ã—224 px, standardized
â”œâ”€â”€ Broken_02.jpg
â”œâ”€â”€ Broken_03.jpg
â”œâ”€â”€ Broken_04.jpg
â”œâ”€â”€ Broken_05.jpg
â””â”€â”€ ... (training images)

processed/test/Broken/
â”œâ”€â”€ Broken_06.jpg            # 224Ã—224 px, standardized
â”œâ”€â”€ Broken_07.jpg
â””â”€â”€ ... (test images)
```

---

## ðŸ”„ Raw vs. Processed: Why Two Folders?

### Key Differences

| Aspect | Raw | Processed |
|--------|-----|-----------|
| **Source** | Original dataset | Train/Test split from raw |
| **Modification** | None - untouched | Heavy preprocessing + split |
| **Resolution** | 500Ã—500 pixels | 224Ã—224 pixels |
| **Train/Test Split** | Not split | 80/20 stratified split |
| **File Size** | Original size | Reduced (smaller resolution) |
| **Use Case** | Reference/Archive | Model training & evaluation |
| **Access Pattern** | Infrequent | Frequent (every epoch) |
| **Storage** | Long-term archival | Active working directory |

### Advantages of This Separation

#### 1. **Data Integrity & Reproducibility**
- **Raw folder**: Preserves original data untouched for verification
- **Processed folder**: Consistent preprocessing for repeatable results
- Enables comparison against source dataset
- Supports academic reproducibility standards
- Facilitates data provenance tracking

#### 2. **Efficient Development Workflow**
- **Raw data**: One-time organization (already done)
- **Processed data**: Used for all training experiments
- Separates data preparation from model development
- Allows parallel experimentation on processed data
- Reduces preprocessing overhead (done once, used many times)

#### 3. **Flexible Experimentation**
- **Maintain raw data** while experimenting with different preprocessing
- Test various image sizes without re-organizing raw data
- Evaluate different train/test split strategies
- Compare preprocessing approaches
- All without losing the original source

#### 4. **Storage & Performance Optimization**
- **Raw folder**: Archived for reference (~1-2GB)
- **Processed folder**: Optimized for training (~200-300MB)
- Faster I/O during training (smaller image files)
- Reduces time per epoch
- Efficient disk space usage

#### 5. **Quality Control**
- **Raw folder**: Validation against source integrity
- **Processed folder**: Verification of preprocessing correctness
- Enables detection of corrupted or invalid images
- Allows debugging of preprocessing issues
- Facilitates train/test split verification

#### 6. **Multi-Experiment Support**
- Different models may need different input sizes
- Can maintain multiple processed versions simultaneously
- Raw data remains as single source of truth
- Scales to multiple preprocessing strategies
- Enables A/B testing of preprocessing approaches

---

## ðŸ“¥ Data Pipeline

### Complete Training Data Flow

```
1. ORIGINAL DATA
   â””â”€> data/raw/
       â””â”€> 17 defect class folders
           â””â”€> Original 500Ã—500 images

2. PREPROCESSING & SPLIT
   â””â”€> Read from raw/
       â”œâ”€> Resize to 224Ã—224
       â”œâ”€> Standardize format (JPG)
       â”œâ”€> Split: 80% train / 20% test
       â””â”€> Write to processed/train/ and processed/test/

3. MODEL TRAINING
   â””â”€> Read from processed/train/
       â”œâ”€> Load batch of images
       â”œâ”€> Apply data augmentation (random rotations, flips, etc.)
       â”œâ”€> Normalize pixel values (mean/std)
       â”œâ”€> Create tensors
       â””â”€> Feed to neural network

4. MODEL EVALUATION
   â””â”€> Read from processed/test/
       â”œâ”€> Load batch of images
       â”œâ”€> NO augmentation (true evaluation)
       â”œâ”€> Normalize pixel values (same as training)
       â”œâ”€> Create tensors
       â””â”€> Evaluate model accuracy
```

---

## ðŸ“Š Dataset Statistics

### Class Distribution

Each defect class has multiple images distributed across train/test sets:

```
Example Distribution (approximate):
- Broken: ~100 images total (80 train, 20 test)
- Cut: ~100 images total (80 train, 20 test)
- Dry Cherry: ~100 images total (80 train, 20 test)
- ... (consistent ~100 per class)
- Total: ~1,700 images (1,360 train, 340 test)

Note: Exact counts may vary based on original dataset
```

### Storage Requirements

```
Raw Data:
- ~1-2 GB total (500Ã—500 images, original quality)

Processed Data:
- ~200-300 MB (224Ã—224 resized images)
- train/: ~160-240 MB
- test/: ~40-60 MB

Combined:
- Total disk space needed: ~1.5-2.5 GB
```

---

## ðŸš€ Using the Data in Training

### Loading Data in Python

```python
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Define transforms for training
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# Define transforms for testing (no augmentation)
test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# Load training dataset
train_dataset = datasets.ImageFolder('data/processed/train/', transform=train_transforms)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Load testing dataset
test_dataset = datasets.ImageFolder('data/processed/test/', transform=test_transforms)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Verify class mapping
print(f"Classes: {train_dataset.classes}")
print(f"Number of classes: {len(train_dataset.classes)}")
```