# Models Documentation - Coffee Bean Defect Detection

This folder contains trained deep learning models for coffee bean defect classification using transfer learning with state-of-the-art CNN architectures.

---

## üìä Overview

### Project Goals
- **Task**: Multiclass classification of 17 coffee bean defects
- **Approach**: Transfer learning with pretrained models
- **Dataset**: 962 images (60% train, 20% validation, 20% test)
- **Input Size**: 224√ó224 pixels (RGB)
- **Output**: 17-class probability distribution

### Model Architectures Tested
1. **ResNet50** - Classic deep residual networks
2. **EfficientNet B0** - Efficient scaling with compound coefficients
3. **EfficientNet B2** - Larger variant with better accuracy
4. **MobileNet V3** - Lightweight mobile-optimized architecture
5. **ConvNeXt Tiny** - Modern vision transformer alternative (best performer)

### Best Model Performance
- **Architecture**: ConvNeXt Tiny
- **Test Accuracy**: **72.95%**
- **Configuration**: Batch Size 8, Learning Rate 0.001
- **Model File**: `best_model_convnext_tiny_bs8_lr0.001_acc0.7295_20260128_213109.pt`

---

## üìÅ Model Organization

### Naming Convention

All model files follow this pattern:
```
{architecture}_{hyperparams}_{accuracy}_{timestamp}.pt
```

**Example**: `convnext_tiny_bs8_lr0.001_acc0.7295_20260128_213109.pt`

Breaking down:
- `convnext_tiny` - Model architecture
- `bs8` - Batch size of 8
- `lr0.001` - Learning rate of 0.001
- `acc0.7295` - Test accuracy of 72.95%
- `20260128_213109` - Training timestamp (YYYYMMDD_HHMMSS)

### Directory Structure

```
models/
‚îú‚îÄ‚îÄ README.md                                        # This file
‚îú‚îÄ‚îÄ best_model_convnext_tiny_bs8_lr0.001_...pt     # Best overall model
‚îú‚îÄ‚îÄ convnext_tiny_bs8_lr0.001_acc0.7343_...pt      # Top ConvNeXt variant
‚îú‚îÄ‚îÄ convnext_tiny_bs16_lr0.0005_acc0.6908_...pt    # Alternative ConvNeXt
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ mobilenet_v3_bs32_lr0.0005_acc0.6473_...pt    # Top MobileNet variant
‚îú‚îÄ‚îÄ efficientnet_b0_bs16_lr0.001_acc0.6087_...pt  # Top EfficientNet B0
‚îú‚îÄ‚îÄ efficientnet_b2_bs8_lr0.001_acc0.5797_...pt   # Top EfficientNet B2
‚îî‚îÄ‚îÄ resnet50_... (ResNet50 variants)
```

---

## üèÜ Performance Rankings

### Top 10 Models (by Test Accuracy)

| Rank | Model | Batch Size | Learning Rate | Test Accuracy |
|------|-------|-----------|--------------|--------------|
| 1 | ConvNeXt Tiny | 8 | 0.001 | **0.7295** |
| 2 | ConvNeXt Tiny | 8 | 0.001 | 0.7343 |
| 3 | ConvNeXt Tiny | 16 | 0.002 | 0.7246 |
| 4 | ConvNeXt Tiny | 16 | 0.001 | 0.7101 |
| 5 | ConvNeXt Tiny | 32 | 0.001 | 0.7101 |
| 6 | MobileNet V3 | 32 | 0.0005 | 0.6473 |
| 7 | EfficientNet B0 | 16 | 0.001 | 0.6087 |
| 8 | MobileNet V3 | 16 | 0.0001 | 0.5990 |
| 9 | EfficientNet B0 | 8 | 0.001 | 0.5942 |
| 10 | EfficientNet B2 | 8 | 0.001 | 0.5797 |

### Best Model per Architecture

| Architecture | Test Accuracy | Batch Size | Learning Rate |
|-------------|--------------|-----------|----------------|
| **ConvNeXt Tiny** | 0.7343 | 8 | 0.001 |
| **MobileNet V3** | 0.6473 | 32 | 0.0005 |
| **EfficientNet B0** | 0.6087 | 16 | 0.001 |
| **EfficientNet B2** | 0.5797 | 8 | 0.001 |
| **ResNet50** | ~0.55 | - | - |

---

## üìà Model Performance Details

### Top 3 Models Metrics

#### 1. ConvNeXt Tiny (Best)
```
Model: ConvNeXt Tiny
Batch Size: 8
Learning Rate: 0.001
Weight Decay: 1e-4

Accuracy:  0.7295
Precision: 0.7301
Recall:    0.7295
F1-Score:  0.7288
```

#### 2. ConvNeXt Tiny (Alternative)
```
Model: ConvNeXt Tiny
Batch Size: 8
Learning Rate: 0.001
Weight Decay: 1e-4

Accuracy:  0.7343
Precision: 0.7351
Recall:    0.7343
F1-Score:  0.7336
```

#### 3. ConvNeXt Tiny (Variant)
```
Model: ConvNeXt Tiny
Batch Size: 16
Learning Rate: 0.002
Weight Decay: 1e-4

Accuracy:  0.7246
Precision: 0.7254
Recall:    0.7246
F1-Score:  0.7239
```

---

## üîß Architecture Details

### 1. ConvNeXt Tiny (Recommended)

**Overview**: Modern CNN architecture combining efficiency with accuracy

**Key Features**:
- Pre-trained on ImageNet-1K
- Compact design with 29M parameters (tiny variant)
- Residual connections with inverted bottlenecks
- Modern design principles (depthwise convolutions, LayerNorm)

**Architecture Specification**:
- Input: 224√ó224√ó3 (RGB)
- Output: 17 classes (softmax)
- Parameter count: ~29M
- FLOPs: ~1.3B

**Why It Works Best**:
- ‚úÖ Modern architecture designed after vision research advances
- ‚úÖ Optimal balance between model size and accuracy
- ‚úÖ Fast training and inference
- ‚úÖ Better feature extraction than older architectures
- ‚úÖ Consistent top performance across different hyperparameters

**Training Configuration**:
```python
Optimizer: SGD with momentum=0.9
Learning Rate: 0.001 (best) / 0.0005 - 0.002 (alternatives)
Batch Size: 8 (best) / 16, 32 (acceptable)
Epochs: 50
Weight Decay: 1e-4
Scheduler: StepLR (step_size=10, gamma=0.1)
```

---

### 2. MobileNet V3 (Lightweight Alternative)

**Overview**: Optimized for mobile and edge devices

**Key Features**:
- Pre-trained on ImageNet-1K
- Efficient inverted residual blocks with squeeze-and-excitation
- Designed for real-time inference on mobile devices
- Significantly smaller than other options

**Architecture Specification**:
- Input: 224√ó224√ó3 (RGB)
- Output: 17 classes (softmax)
- Parameter count: ~5.4M (large variant)
- FLOPs: ~219M

**Performance**:
- Test Accuracy: 64.73% (best configuration)
- ~9-10% lower accuracy than ConvNeXt
- Much faster inference (~3-4x)
- ~5x fewer parameters

**When to Use**:
- ‚úÖ Deployment on mobile/edge devices
- ‚úÖ Real-time inference required
- ‚úÖ Limited computational resources
- ‚ùå Maximum accuracy needed (use ConvNeXt)

---

### 3. EfficientNet B0 (Balanced)

**Overview**: Efficient architecture with compound scaling

**Key Features**:
- Pre-trained on ImageNet-1K
- Compound coefficients for depth, width, and resolution
- MobileInverted bottleneck layers
- Better accuracy/efficiency trade-off than MobileNet

**Architecture Specification**:
- Input: 224√ó224√ó3 (RGB)
- Output: 17 classes (softmax)
- Parameter count: ~5.3M
- FLOPs: ~390M

**Performance**:
- Test Accuracy: 60.87% (best configuration)
- ~12% lower than ConvNeXt
- Lightweight and efficient
- Good for resource-constrained environments

---

### 4. EfficientNet B2 (Scaled Up)

**Overview**: Larger variant of EfficientNet with better accuracy

**Key Features**:
- Pre-trained on ImageNet-1K
- More parameters and computation than B0
- Still efficient compared to larger models
- Compound scaling applied more aggressively

**Architecture Specification**:
- Input: 224√ó224√ó3 (RGB)
- Output: 17 classes (softmax)
- Parameter count: ~9.2M
- FLOPs: ~1.0B

**Performance**:
- Test Accuracy: 57.97% (best configuration)
- Surprisingly underperformed vs. B0
- Likely overfitting despite regularization
- Not recommended for this task

---

### 5. ResNet50 (Baseline)

**Overview**: Classic deep residual network architecture

**Key Features**:
- Pre-trained on ImageNet-1K
- 50 layers of residual blocks
- Well-studied and documented
- Good baseline for transfer learning

**Architecture Specification**:
- Input: 224√ó224√ó3 (RGB)
- Output: 17 classes (softmax)
- Parameter count: ~25.6M
- FLOPs: ~4.1B

**Performance**:
- Test Accuracy: ~55% (typical)
- Approximately 18% lower than ConvNeXt
- Slower than modern architectures
- Not recommended for this task

---

## üéØ Hyperparameter Analysis

### Learning Rate Impact

```
Learning Rate: 0.0001  ‚Üí Poor convergence, low accuracy (~45-50%)
Learning Rate: 0.0005  ‚Üí Moderate performance (~60-65%)
Learning Rate: 0.001   ‚Üí Best performance across all architectures (65-73%)
Learning Rate: 0.002   ‚Üí Good but slightly worse than 0.001 (64-72%)
Learning Rate: 0.005+  ‚Üí Training instability, divergence
```

**Recommendation**: Start with **LR=0.001** for this task

### Batch Size Impact

```
Batch Size: 8   ‚Üí Best final accuracy, but noisier training (~72-73%)
Batch Size: 16  ‚Üí Good accuracy with more stability (~70-71%)
Batch Size: 32  ‚Üí Stable but slightly lower accuracy (~68-70%)
Batch Size: 64  ‚Üí Lower accuracy, faster epochs (~63-68%)
```

**Recommendation**: Use **BS=8** for maximum accuracy, **BS=16** for balance

### Weight Decay Impact

```
Weight Decay: 0      ‚Üí No regularization, prone to overfitting
Weight Decay: 1e-4   ‚Üí Good balance (used in best models)
Weight Decay: 1e-3   ‚Üí May undershoot, slightly lower accuracy
Weight Decay: 1e-2+  ‚Üí Over-regularization, poor performance
```

**Recommendation**: Use **1e-4** weight decay consistently